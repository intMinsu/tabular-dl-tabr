{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9007d2ac-8a0e-4d95-b702-e5aaff5dba37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "_project_dir = os.path.dirname(os.getcwd())\n",
    "os.environ['PROJECT_DIR'] = _project_dir\n",
    "sys.path.append(_project_dir)\n",
    "del _project_dir\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional, Union, cast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import lib; lib.configure_libraries()\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2799e6f-7aba-42df-85a3-4961b9f02638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PROPERTIES = [\"task_type\", \"size\", \"n_features\"]\n",
    "_DATASETS_INFO: dict[Path, dict[str, Any]] = {}\n",
    "\n",
    "DATASETS_MAIN = [\n",
    "    'sait',\n",
    "]\n",
    "# The datasets from the paper \"Why do tree-based models still outperform deep learning on tabular data?\"\n",
    "DATASETS_WHY = [\n",
    "    '',\n",
    "]\n",
    "\n",
    "DATASETS_ALL = DATASETS_MAIN\n",
    "\n",
    "\n",
    "def get_dataset_info(dpath: Union[str, Path]) -> dict:\n",
    "    dpath = lib.get_path(dpath)\n",
    "    if dpath in _DATASETS_INFO:\n",
    "        return _DATASETS_INFO[dpath]\n",
    "\n",
    "    dataset = lib.Dataset.from_dir(dpath, None)\n",
    "    _DATASETS_INFO[dpath] = {\n",
    "        'dataset': (\n",
    "            dpath.name.upper()[:2] if dpath.parent == lib.DATA_DIR and dpath.name in DATASETS_MAIN\n",
    "            else 'WE (full)' if dpath.parent == lib.DATA_DIR and dpath.name == 'weather-big'\n",
    "            else dpath.name\n",
    "        ),\n",
    "        'task_type': dataset.task_type.value,\n",
    "        'size': dataset.size(None),\n",
    "        'n_features': dataset.n_features,   \n",
    "    }\n",
    "    return deepcopy(_DATASETS_INFO[dpath])\n",
    "\n",
    "\n",
    "def load_record(output: Union[str, Path]):\n",
    "    output = lib.get_path(output)\n",
    "    report = lib.load_report(output)\n",
    "    if lib.EXP_DIR in output.parents and '/exp/npt/' in str(output):\n",
    "        # The NPT reports do not follow the required format,\n",
    "        # so we infer the dataset path from the output path.\n",
    "        dpath = ':data/' + list(output.relative_to(lib.EXP_DIR / 'npt').parents)[-2].name\n",
    "    else:\n",
    "        if report[\"function\"] == 'bin.tune.main':\n",
    "            report = report[\"best\"]\n",
    "\n",
    "        if report[\"function\"] == 'bin.ensemble.main':\n",
    "            dpath = report[\"data\"]\n",
    "        else:\n",
    "            data = report[\"config\"][\"data\"]\n",
    "            dpath = data if isinstance(data, str) else data['path']\n",
    "            del data\n",
    "\n",
    "    record = get_dataset_info(dpath)\n",
    "    for part in lib.Part:\n",
    "        if part.value in report[\"metrics\"]:\n",
    "            score = report[\"metrics\"][part.value][\"score\"]\n",
    "            if record['dataset'] == 'HO':\n",
    "                # Prettify the score for \":data/house\".\n",
    "                score /= 10000\n",
    "            record[f\"{part.value}_score\"] = score\n",
    "    return record\n",
    "\n",
    "\n",
    "def _compute_ranks(dataset_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataset_df = dataset_df.sort_values(['test_mean', 'test_std'], ascending=[False, True])\n",
    "    ranks = []\n",
    "    current_score = None\n",
    "    current_std = None\n",
    "    for _, columns in dataset_df.iterrows():\n",
    "        score = columns['test_mean']\n",
    "        std = columns['test_std']\n",
    "        if current_score is None:\n",
    "            ranks.append(1)\n",
    "            current_score = score\n",
    "            current_std = std\n",
    "        elif current_score - score <= current_std:\n",
    "            ranks.append(ranks[-1])\n",
    "        else:\n",
    "            ranks.append(ranks[-1] + 1)\n",
    "            current_score = score\n",
    "            current_std = std\n",
    "    dataset_df['rank'] = ranks\n",
    "    return dataset_df\n",
    "\n",
    "\n",
    "def build_metrics_dataframe(\n",
    "    outputs_info: list[\n",
    "        tuple[\n",
    "            Union[str, Path],  # output path\n",
    "            str,  # key (for example, algorithm name: \"MLP\")\n",
    "            Union[int, str],  # subkey for aggregation (for example, seed: 0)\n",
    "        ]\n",
    "    ],\n",
    "    precision: Optional[int] = 4,\n",
    "):\n",
    "    # >>> Build dataframe.\n",
    "    records = [\n",
    "        load_record(output) | { 'key': key, 'subkey': str(subkey)}\n",
    "        for output, key, subkey in outputs_info\n",
    "        if lib.get_path(output).joinpath('DONE').exists()\n",
    "    ]\n",
    "    if not records:\n",
    "        raise RuntimeError('No records are available')\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    has_train_score = 'train_score' in df.columns\n",
    "\n",
    "    # >>> Aggregate over subkeys.\n",
    "    aggregations = {\n",
    "        'test_mean': (\"test_score\", \"mean\"),\n",
    "        'test_std': (\"test_score\", \"std\"),\n",
    "        'val_mean': (\"val_score\", \"mean\"),\n",
    "        'val_std': (\"val_score\", \"std\"),\n",
    "    }\n",
    "    if has_train_score:\n",
    "        aggregations.update({\n",
    "            'train_mean': (\"train_score\", \"mean\"),\n",
    "            'train_std': (\"train_score\", \"std\"),\n",
    "        })\n",
    "    aggregations['count'] = (\"test_score\", \"count\")\n",
    "    aggregations.update({\n",
    "        x: (x, \"first\")\n",
    "        for x in DATASET_PROPERTIES\n",
    "        if x in df.columns\n",
    "    })\n",
    "    df = df.groupby([\"dataset\", \"key\"]).agg(**aggregations)\n",
    "    df = df.reset_index().fillna(0.0)\n",
    "    df[\"count\"] = df[\"count\"].astype(int)\n",
    "\n",
    "    # >>> Compute ranks.\n",
    "    df = cast(\n",
    "        pd.DataFrame,\n",
    "        df.groupby(['dataset'], group_keys=False).apply(_compute_ranks)\n",
    "    )\n",
    "\n",
    "    # >>> Finalize.\n",
    "    df = df.sort_values(\n",
    "        ['size', 'n_features', 'dataset', 'test_mean'],\n",
    "        ascending=[True, True, True, False],\n",
    "    ).reset_index(drop=True)\n",
    "    df.loc[\n",
    "        df['task_type'] == 'regression',\n",
    "        ['test_mean', 'val_mean'] + ['train_mean'] * int(has_train_score)\n",
    "    ] *= -1\n",
    "    if precision is not None:\n",
    "        float_columns = [\n",
    "            'test_mean', 'test_std',\n",
    "            'val_mean', 'val_std',\n",
    "        ] + ['train_mean', 'train_std'] * int(has_train_score)\n",
    "        df[float_columns] = df[float_columns].round(precision)\n",
    "    df = df.set_index([\"dataset\"] + DATASET_PROPERTIES + [\"key\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_ranks(metrics_df: pd.DataFrame, nans: bool) -> pd.DataFrame:\n",
    "    df = metrics_df\n",
    "    df = df.reset_index().pivot(index='key', columns='dataset', values='rank')\n",
    "    if not nans:\n",
    "        df = df.dropna(axis='columns')\n",
    "    columns = df.columns.tolist()\n",
    "    df[\"avg\"] = df.mean(1)\n",
    "    df[\"std\"] = df.std(1)\n",
    "    df.insert(0, \"avg\", df.pop(\"avg\").round(1))\n",
    "    df.insert(1, \"std\", df.pop(\"std\").round(1))\n",
    "    df = df.sort_values(\"avg\")\n",
    "    df = df[['avg', 'std'] + columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fd0b3-0b13-4b7a-8bed-2cec81dad060",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How to use the next cell\n",
    "- comment/uncomment `N_SEEDS += 15` to show/hide results for single models\n",
    "- comment/uncomment `N_ENSEMBLES += 3` to show/hide results for ensembles\n",
    "- in the `for dataset in datasets` loop:\n",
    "    - comment/uncomment the `add(...)` lines to show/hide results for various algorithms\n",
    "    - in particular, uncomment `add(f':exp/mlp/{dataset}/0-reproduce', 'MLP (reproduce)')` to complete the tutorial from `README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b900ea-8a57-4e9b-ac2d-3021f3d19882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SEEDS += 15\n",
    "# N_SEEDS += 15\n",
    "N_ENSEMBLES += 3\n",
    "# N_ENSEMBLES += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ec4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the comments in build_metrics_dataframe to learn about outputs_info.\n",
    "outputs_info = []\n",
    "def add(location: str, name: Optional[str] = None, sep: str = '-'):\n",
    "    if name is None:\n",
    "        assert location.startswith(':exp/')\n",
    "        # location example: \":exp/mlp/california/0\"\n",
    "        _exp_prefix, alg, *_dataset, tag = location.split('/')\n",
    "        name = f'{alg}[{tag}]'\n",
    "    for seed in range(N_SEEDS):\n",
    "        outputs_info.append((location + f'{sep}evaluation/{seed}', name, seed))\n",
    "    for ensemble_i in range(N_ENSEMBLES):\n",
    "        outputs_info.append((location + f'{sep}ensemble{sep}5/{ensemble_i}', '(E) ' + name, ensemble_i))\n",
    "\n",
    "datasets = DATASETS_MAIN\n",
    "for dataset in datasets:\n",
    "\n",
    "    # >>> Tutorial from README.md\n",
    "    # add(f':exp/mlp/{dataset}/0-reproduce', 'MLP (reproduce)')\n",
    "\n",
    "    # >>> Retrieval-augmented baselines\n",
    "    # add(f':exp/knn/{dataset}/0', 'kNN')\n",
    "\n",
    "    # dnnr_tag = 'ohe' if dataset in [BLACK_FRIDAY, DIAMOND] else 'loo'\n",
    "    # add(f':exp/dnnr/{dataset}/{dnnr_tag}', 'DNNR')\n",
    "\n",
    "    # add(f':exp/anp/{dataset}/0', 'ANP')\n",
    "    # add(f':exp/dkl/{dataset}/0', 'DKL')\n",
    "\n",
    "    # npt_tag = {\n",
    "    #     'churn': 0,\n",
    "    #     'california': 0,\n",
    "    #     'house': 0,\n",
    "    #     'adult': 0,\n",
    "    #     'diamond': 2,\n",
    "    #     'otto': 1,\n",
    "    #     'higgs-small': 2,\n",
    "    #     'black-friday': 2,\n",
    "    #     'covtype': 3,\n",
    "    #     'weather-small': 1,\n",
    "    #     'microsoft': 1,\n",
    "    # }[dataset]\n",
    "    # add(f':exp/npt/{dataset}/{npt_tag}', 'NPT')\n",
    "\n",
    "    # saint_tag = 'default' if dataset in ['weather-small', 'covtype', 'microsoft'] else '2'\n",
    "    # add(f':exp/saint/{dataset}/{saint_tag}', 'SAINT')\n",
    "\n",
    "    # >>> Parametric DL baselines\n",
    "    # add(f':exp/mlp/{dataset}/0', 'MLP')\n",
    "    # add(f':exp/mlp/{dataset}/lr', 'MLP-LR')\n",
    "    # add(f':exp/mlp/{dataset}/plr-lite', 'MLP-PLR(lite)')\n",
    "    # add(f':exp/mlp/{dataset}/plr', 'MLP-PLR')\n",
    "\n",
    "    # >>> GBDT\n",
    "    # add(f':exp/xgboost_/{dataset}/default2', 'XGBoost (default)')\n",
    "    # add(f':exp/lightgbm_/{dataset}/default2', 'LightGBM (default)')\n",
    "    # add(f':exp/catboost_/{dataset}/default2', 'CatBoost (default)')\n",
    "\n",
    "    add(f':exp/xgboost_/{dataset}/2', 'XGBoost')\n",
    "    add(f':exp/lightgbm_/{dataset}/2', 'LightGBM')\n",
    "    add(f':exp/catboost_/{dataset}/2', 'CatBoost')\n",
    "\n",
    "    # >>> The model\n",
    "    model = 'TabR'\n",
    "    modeldir = model.lower()\n",
    "    # add(f':exp/{modeldir}/{dataset}/default', f'{model}-S (default)')\n",
    "    # add(f':exp/{modeldir}/{dataset}/0', f'{model}-S')\n",
    "    model_tag = \"2-lr\" if dataset in ['weather-small', 'covtype', 'microsoft'] else \"2-plr-lite\"\n",
    "    add(f':exp/{modeldir}/{dataset}/{model_tag}', f'{model}')\n",
    "\n",
    "    # >>> Ablation study\n",
    "    for tag, name in [\n",
    "        # ('dp-qk-v-self-scaled', 'Step-0'),\n",
    "        # ('dp-qk-yv-self-scaled', 'Step-1'),\n",
    "        # ('l2-k-yv-self-scaled', 'Step-2'),\n",
    "        # ('l2-k-yt-self-scaled', 'Step-3'),\n",
    "    ]:\n",
    "        add(f':exp/{modeldir}_design/{dataset}/{tag}', f'(design) {name}')\n",
    "\n",
    "    # >>> Context freeze\n",
    "    for freeze_after_n_epochs in [\n",
    "        # 0,\n",
    "        # 1,\n",
    "        # 2,\n",
    "        # 4,\n",
    "        # 5,\n",
    "        # 8,\n",
    "    ]:\n",
    "        add(f':exp/{modeldir}_scaling/{dataset}/default-freeze-{freeze_after_n_epochs}', f'{model}-freeze-{freeze_after_n_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd88a607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cccf68",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No records are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_metrics_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Drop details about datasets to save screen space.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# while len(metrics_df.index.levels) > 2:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     metrics_df.index = metrics_df.index.droplevel(1)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ranks_df \u001b[38;5;241m=\u001b[39m summarize_ranks(metrics_df, nans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 102\u001b[0m, in \u001b[0;36mbuild_metrics_dataframe\u001b[0;34m(outputs_info, precision)\u001b[0m\n\u001b[1;32m     96\u001b[0m records \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     97\u001b[0m     load_record(output) \u001b[38;5;241m|\u001b[39m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m: key, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubkey\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(subkey)}\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output, key, subkey \u001b[38;5;129;01min\u001b[39;00m outputs_info\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mget_path(output)\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mexists()\n\u001b[1;32m    100\u001b[0m ]\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m records:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo records are available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    103\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(records)\n\u001b[1;32m    104\u001b[0m has_train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_score\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No records are available"
     ]
    }
   ],
   "source": [
    "metrics_df = build_metrics_dataframe(outputs_info)\n",
    "# Drop details about datasets to save screen space.\n",
    "# while len(metrics_df.index.levels) > 2:\n",
    "#     metrics_df.index = metrics_df.index.droplevel(1)\n",
    "ranks_df = summarize_ranks(metrics_df, nans=True)\n",
    "print('Ranks:')\n",
    "display(ranks_df)\n",
    "print('\\nMetrics:')\n",
    "display(metrics_df)\n",
    "# metrics_df.to_html('metrics.html')\n",
    "# ranks_df.to_html('metrics.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fda47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad1c4ccb4b0f87320904f04a0667e53dc3875ea3cc5dc70aa6dac74e1b6b3256"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(np.load('data/sait/Y_test.npy'))\n",
    "y_test_pred_df = pd.DataFrame(np.load('exp/tabr/sait/2-config01-plr-lite-evaluation/0/our-prediction.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 82.83026885986328\n",
      "Normalized RMSE (NRMSE): [2.4443264]\n",
      "Mean Absolute Error (MAE): 82.78702545166016\n",
      "R-squared (R2): -941.2960035741698\n"
     ]
    }
   ],
   "source": [
    "y_test_df.iloc# Make sure the shapes of the two dataframes match\n",
    "assert y_test_df.shape == y_test_pred_df.shape, \"Shape mismatch between test and prediction dataframes.\"\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_df, y_test_pred_df))\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test_df, y_test_pred_df)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test_df, y_test_pred_df)\n",
    "\n",
    "# Calculate the range of the true values (max - min)\n",
    "y_min = y_test_df.min().values\n",
    "y_max = y_test_df.max().values\n",
    "range_y = y_max - y_min\n",
    "\n",
    "# Calculate the Normalized RMSE (by range)\n",
    "nrmse = rmse / range_y\n",
    "\n",
    "# Print the results\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Normalized RMSE (NRMSE): {nrmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated submission saved to real-submission.csv\n"
     ]
    }
   ],
   "source": [
    "# submission          \n",
    "submission_df = pd.read_csv('sample_submission.csv')     \n",
    "submission_df['y'] = y_test_pred_df.iloc[:]\n",
    "submission_df.to_csv('real-submission.csv', index=False)\n",
    "print(f\"Updated submission saved to real-submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabr-cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
